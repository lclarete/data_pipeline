{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import vaex.jupyter.model as vjm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liviaclarete/.pyenv/versions/coca3/lib/python3.7/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'pt_core_news_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from data_pipeline.modeling.my_topicmodeling import lda_topic_model\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, file_name):\n",
    "    \"\"\"\n",
    "    Input: path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output: list of paragraphs/ documents and title\n",
    "    (initial 100 words considered as title of document)\n",
    "    \"\"\"\n",
    "    \n",
    "    documents_list = []\n",
    "    titles = []\n",
    "    \n",
    "    with open(os.path.join(path, file_name), 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print('Total Number of Documets', len(documents_list))\n",
    "    \n",
    "    titles.append(text[0:min(len(text), 100)])\n",
    "    \n",
    "    return documents_list, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documets 4551\n"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "file_name = 'articles+4.txt'\n",
    "data = load_data(path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "* tokenize the text articles\n",
    "* remove stop words\n",
    "* perform stemming on text articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input: document list\n",
    "    Purpose: preprocess text (tokenize, remove stopwords, stemming)\n",
    "    Ouput: preprocess text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = preprocess_data(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare corpus\n",
    "* Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input: clean document\n",
    "    Purpose: create term dictionary of our corpus and converting\n",
    "    list of documents (corpus) into Document Term Matrix\n",
    "    Output: term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # creating the term dictionary of our corpus, where every unique\n",
    "    # term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # converting list of documents (corpus) into Document Term Matrix \n",
    "    # using dictionary prepared above\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary, doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:MainThread:gensim.corpora.dictionary:built Dictionary(49792 unique tokens: ['11', '14bn', '1bn', '2005', '2007']...) from 4551 documents (total 2382025 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary, doc_term_matrix = prepare_corpus(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an LSA model using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean, number_of_topics, words):\n",
    "    \"\"\"\n",
    "    Input: clean document, number of topics and number of words\n",
    "    associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output: return LSA model\n",
    "    \"\"\"\n",
    "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
    "\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, \n",
    "                        num_topics=number_of_topics, \n",
    "                        id2word = dictionary)  \n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:MainThread:gensim.corpora.dictionary:built Dictionary(49792 unique tokens: ['11', '14bn', '1bn', '2005', '2007']...) from 4551 documents (total 2382025 corpus positions)\n",
      "INFO:MainThread:gensim.models.lsimodel:using serial LSI version on this node\n",
      "INFO:MainThread:gensim.models.lsimodel:updating model with new documents\n",
      "INFO:MainThread:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:MainThread:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:MainThread:gensim.models.lsimodel:1st phase: constructing (49792, 107) action matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:orthonormalizing (49792, 107) action matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:2nd phase: running dense svd on (107, 4551) matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:MainThread:gensim.models.lsimodel:keeping 7 factors (discarding 33.285% of energy spectrum)\n",
      "INFO:MainThread:gensim.models.lsimodel:processed documents up to #4551\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #0(1672.211): 0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #1(1032.220): 0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #2(898.751): 0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #3(657.909): -0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #4(507.995): 0.404*\"bank\" + -0.305*\"eu\" + -0.290*\"min\" + 0.189*\"year\" + -0.164*\"leav\" + -0.153*\"cameron\" + 0.143*\"market\" + 0.140*\"rate\" + -0.139*\"vote\" + -0.133*\"say\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #0(1672.211): 0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #1(1032.220): 0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #2(898.751): 0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #3(657.909): -0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #4(507.995): 0.404*\"bank\" + -0.305*\"eu\" + -0.290*\"min\" + 0.189*\"year\" + -0.164*\"leav\" + -0.153*\"cameron\" + 0.143*\"market\" + 0.140*\"rate\" + -0.139*\"vote\" + -0.133*\"say\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #5(457.865): -0.310*\"bank\" + 0.307*\"say\" + 0.221*\"peopl\" + -0.203*\"trump\" + -0.166*\"1\" + -0.164*\"min\" + -0.163*\"0\" + -0.152*\"market\" + -0.152*\"eu\" + 0.138*\"like\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #6(410.717): 0.570*\"say\" + 0.237*\"min\" + -0.170*\"vote\" + 0.158*\"govern\" + -0.154*\"poll\" + 0.122*\"tax\" + 0.115*\"statement\" + 0.115*\"bank\" + 0.112*\"budget\" + -0.108*\"one\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"'), (1, '0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"'), (2, '0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"'), (3, '-0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"'), (4, '0.404*\"bank\" + -0.305*\"eu\" + -0.290*\"min\" + 0.189*\"year\" + -0.164*\"leav\" + -0.153*\"cameron\" + 0.143*\"market\" + 0.140*\"rate\" + -0.139*\"vote\" + -0.133*\"say\"'), (5, '-0.310*\"bank\" + 0.307*\"say\" + 0.221*\"peopl\" + -0.203*\"trump\" + -0.166*\"1\" + -0.164*\"min\" + -0.163*\"0\" + -0.152*\"market\" + -0.152*\"eu\" + 0.138*\"like\"'), (6, '0.570*\"say\" + 0.237*\"min\" + -0.170*\"vote\" + 0.158*\"govern\" + -0.154*\"poll\" + 0.122*\"tax\" + 0.115*\"statement\" + 0.115*\"bank\" + 0.112*\"budget\" + -0.108*\"one\"')]\n"
     ]
    }
   ],
   "source": [
    "lsa_model = create_gensim_lsa_model(data_clean, 7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\d\n",
      "<ipython-input-142-958edee04577>:2: DeprecationWarning: invalid escape sequence \\d\n",
      "  n = re.findall('\\d.\\d{3}', t)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.532"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lsa_model.print_topic(1)\n",
    "n = re.findall('\\d.\\d{3}', t)\n",
    "n = list(map(lambda x: float(x), n))\n",
    "sum(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        dictionary: Gensim dictionary\n",
    "        corpus: Gensim corpus\n",
    "        text: List of input texts\n",
    "        stop: Max num of topics\n",
    "    Purpose: compute c_v coherence for various number of topics\n",
    "    Output: \n",
    "        model_list: List of LSA topic models\n",
    "        coherence_values: Coherence values corresponding to the \n",
    "                LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        # train the model\n",
    "        model = LsiModel(doc_term_matrix, \n",
    "                         num_topics=num_topics,\n",
    "                         id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherence_values = CoherenceModel(model=model, \n",
    "                                          text=doc_clean, \n",
    "                                          dictionary=dictionary,\n",
    "                                          coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the coherence score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean, start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start,stop,step=2,12,1\n",
    "# plot_graph(data_clean,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documets 4551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:MainThread:gensim.corpora.dictionary:built Dictionary(49792 unique tokens: ['11', '14bn', '1bn', '2005', '2007']...) from 4551 documents (total 2382025 corpus positions)\n",
      "INFO:MainThread:gensim.models.lsimodel:using serial LSI version on this node\n",
      "INFO:MainThread:gensim.models.lsimodel:updating model with new documents\n",
      "INFO:MainThread:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:MainThread:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:MainThread:gensim.models.lsimodel:1st phase: constructing (49792, 107) action matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:orthonormalizing (49792, 107) action matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:2nd phase: running dense svd on (107, 4551) matrix\n",
      "INFO:MainThread:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:MainThread:gensim.models.lsimodel:keeping 7 factors (discarding 33.298% of energy spectrum)\n",
      "INFO:MainThread:gensim.models.lsimodel:processed documents up to #4551\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #0(1672.211): 0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #1(1032.220): 0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #2(898.751): 0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #3(657.909): -0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #4(507.995): -0.404*\"bank\" + 0.305*\"eu\" + 0.290*\"min\" + -0.189*\"year\" + 0.164*\"leav\" + 0.153*\"cameron\" + -0.143*\"market\" + -0.140*\"rate\" + 0.139*\"vote\" + 0.133*\"say\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #0(1672.211): 0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #1(1032.220): 0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #2(898.751): 0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #3(657.909): -0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #4(507.995): -0.404*\"bank\" + 0.305*\"eu\" + 0.290*\"min\" + -0.189*\"year\" + 0.164*\"leav\" + 0.153*\"cameron\" + -0.143*\"market\" + -0.140*\"rate\" + 0.139*\"vote\" + 0.133*\"say\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #5(457.865): 0.310*\"bank\" + -0.307*\"say\" + -0.221*\"peopl\" + 0.203*\"trump\" + 0.166*\"1\" + 0.164*\"min\" + 0.164*\"0\" + 0.152*\"market\" + 0.152*\"eu\" + -0.138*\"like\"\n",
      "INFO:MainThread:gensim.models.lsimodel:topic #6(410.717): -0.570*\"say\" + -0.237*\"min\" + 0.170*\"vote\" + -0.158*\"govern\" + 0.154*\"poll\" + -0.122*\"tax\" + -0.115*\"bank\" + -0.115*\"statement\" + -0.112*\"budget\" + 0.108*\"one\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"'), (1, '0.389*\"citi\" + 0.370*\"v\" + 0.356*\"h\" + 0.355*\"2016\" + 0.354*\"2017\" + 0.164*\"unit\" + 0.159*\"west\" + 0.157*\"manchest\" + 0.116*\"apr\" + 0.112*\"dec\"'), (2, '0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"'), (3, '-0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"'), (4, '-0.404*\"bank\" + 0.305*\"eu\" + 0.290*\"min\" + -0.189*\"year\" + 0.164*\"leav\" + 0.153*\"cameron\" + -0.143*\"market\" + -0.140*\"rate\" + 0.139*\"vote\" + 0.133*\"say\"'), (5, '0.310*\"bank\" + -0.307*\"say\" + -0.221*\"peopl\" + 0.203*\"trump\" + 0.166*\"1\" + 0.164*\"min\" + 0.164*\"0\" + 0.152*\"market\" + 0.152*\"eu\" + -0.138*\"like\"'), (6, '-0.570*\"say\" + -0.237*\"min\" + 0.170*\"vote\" + -0.158*\"govern\" + 0.154*\"poll\" + -0.122*\"tax\" + -0.115*\"bank\" + -0.115*\"statement\" + -0.112*\"budget\" + 0.108*\"one\"')]\n"
     ]
    }
   ],
   "source": [
    "# LSA Model\n",
    "number_of_topics=7\n",
    "words=10\n",
    "document_list,titles=load_data(\"\",\"./data/articles+4.txt\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LsiModel' object has no attribute 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6399f2819af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/coca3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/coca3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m           \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m       \u001b[0mdoc_topic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LsiModel' object has no attribute 'inference'"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lsa_model, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten a list\n",
    "flat_text = sum(data_clean, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' '.join(flat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second tutorial:\n",
    "https://www.kdnuggets.com/2018/08/topic-modeling-lsa-plsa-lda-lda2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw document to tf-idf matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD to reduce dimentionality\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline of tf-idf + SVD, fit to and applied to documents\n",
    "svd_transformer = Pipeline(\n",
    "    [('tfidef', vectorizer),\n",
    "     ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd matrix can be later used to compare documents, words or queries\n",
    "svd_matrix = svd_transformer.fit_transform(flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2382025"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svd_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA\n",
    "Rarely used in real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Latent Dirichlet Allocation. It is a Bayesian version of pLSA.\n",
    "\n",
    "\n",
    "I like to draw an analogy between the Dirichlet Distribution and the normal distribution, since most people understand the normal distribution.\n",
    "\n",
    "The normal distribution is a probability distribution over all the real numbers. It is described by a mean and a variance. The mean is the expected value of this distribution, and the variance tells us how much we can expect samples to deviate from the mean. If the variance is very high, then you’re going to see values that are both much smaller than the mean and much larger than the mean. If the variance is small, then the samples will be very close to the mean. If the variance goes close to zero, all samples will be almost exactly at the mean.\n",
    "\n",
    "The dirichlet distribution is a probability distribution as well - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex.\n",
    "\n",
    "And what is a probability simplex? It’s a bunch of numbers that add up to 1. For example:\n",
    "\n",
    "(0.6, 0.4)\n",
    "(0.1, 0.1, 0.8)\n",
    "(0.05, 0.2, 0.15, 0.1, 0.3, 0.2)\n",
    "\n",
    "These numbers represent probabilities over K distinct categories. In the above examples, K is 2, 3, and 6 respectively. That’s why they are also called categorical distributions.\n",
    "\n",
    "https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution\n",
    "\n",
    "Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process: http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora.dictionary.Dictionary import load_from_text, doc2bow\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:gensim.models.ldamodel:using symmetric alpha at 0.01\n",
      "INFO:MainThread:gensim.models.ldamodel:using symmetric eta at 0.01\n",
      "INFO:MainThread:gensim.models.ldamodel:using serial LDA version on this node\n"
     ]
    }
   ],
   "source": [
    "# extract 100 LDA topics, updating once every 10,000 \n",
    "lda = LdaModel(id2word=dictionary, \n",
    "               num_topics=100, \n",
    "               update_every=1, \n",
    "               chunksize=10000, \n",
    "               passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA model: transform new doc to bag-of-words\n",
    "# then apply lda\n",
    "# doc_bow = Dictionary.doc2bow(document=string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lda2Vec\n",
    "lda2vec is an extension of word2vec and LDA that jointly learns word, document, and topic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
