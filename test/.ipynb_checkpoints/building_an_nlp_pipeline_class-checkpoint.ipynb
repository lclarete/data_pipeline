{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a class to manage our NLP pipelines\n",
    "* https://www.youtube.com/watch?v=SG6jdlBx_vQ\n",
    "    \n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/tree/master/nlp_pipeline_manager\n",
    "\n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/blob/master/nlp_pipeline_manager/pipeline_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's such a pain to manage all the permutations of NLP cleaners/tokenizers/vectorizers/stemmers/etc, we're going to build a class that takes all of those pieces in and manages the pipelines for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'resulting result results resulted resulting run UPPER CASE @you running ran No #results  ðŸ˜º ðŸ˜º ðŸ˜º@results FOUND. View all teams. MAD Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org'\n",
    "s1 = 'run bunda bunda bunda No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb https://www.w3schools.com/python/python_regex.asp'\n",
    "s2 = 'https://www.w3schools.com/python/python_regex.asp No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s3 = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s4 = 'bunda results results found view teams prod fundraistrick ave suite san diego back top donor support'\n",
    "list_of_strings = [s, s1, s2, s3, s4]\n",
    "corpus = list_of_strings\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "dict_regex = {\n",
    "    'hashtags': r'#(\\w+)',\n",
    "    # returns not only mentions, but\n",
    "    # part of the email after the @\n",
    "    'mentions': r'@(\\w+)',\n",
    "    'emails': r'',\n",
    "    'links': r'https?:\\/\\/.*[\\r\\n]*',\n",
    "    'remove_RT': '^RT[\\s]+',\n",
    "    'numbers': r'\\d+',\n",
    "    'symbols': r'',\n",
    "    'punctionation2': '[^\\w\\s]',\n",
    "    'punctionation': '[%s]' % re.escape(string.punctuation),\n",
    "    'periods': '\\.',\n",
    "    'exclamation points': '\\!',\n",
    "    'question marks': '\\?',\n",
    "    'upper case words': '[A-Z][A-Z\\d]+',\n",
    "    # https://stackoverflow.com/questions/39536390/match-unicode-emoji-in-python-regex\n",
    "    'emojis': '\\d+(.*?)[\\u263a-\\U0001f645]',\n",
    "    'emojis_work': \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\",\n",
    "    'upper case': '[A-Z][A-Z\\d]+'\n",
    "}\n",
    "\n",
    "regex_emojis = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "list_of_regex_values = list(dict_regex.values())\n",
    "list_of_regex_keys = list(dict_regex.keys())\n",
    "\n",
    "sw = ['ðŸ˜º', 'ðŸ˜º ðŸ˜º', 'ðŸ˜º ðŸ˜º ðŸ˜º', 'prod', 'suite', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:53.819932Z",
     "start_time": "2018-08-12T19:36:52.882108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.6 (default, Jan  7 2020, 16:28:00) \n",
      "[Clang 11.0.0 (clang-1100.0.33.8)] \n",
      "\n",
      "Matplotlib Version: 3.2.0\n",
      "Numpy Version: 1.18.1\n",
      "Pandas Version: 0.25.3\n",
      "NLTK Version: 3.5\n",
      "sklearn Version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "# arrays and tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "# viz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numbers and calculation\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "# files\n",
    "import sys\n",
    "\n",
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('NLTK', nltk), ('sklearn',sklearn))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pipeline: preprocessing and supervised nlp\n",
    "from data_pipeline.pre_processing_text.class_preprocessing import nlp_preprocessor\n",
    "from data_pipeline.modeling.class_supervised_ml import supervised_nlp\n",
    "from data_pipeline.modeling.my_topicmodeling import topic_modeling_nlp\n",
    "\n",
    "# lemmatize and stem words\n",
    "from data_pipeline.pre_processing_text.norm_lemmatize import lemmatize_list\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "nlp = nlp_preprocessor(lemmatizer=lemmatize_list)\n",
    "# nlp = nlp_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resulting', 'result', 'results', 'resulted', 'resulting', 'run', 'UPPER', 'CASE', '@you', 'running', 'ran', 'No', '#results', '', 'ðŸ˜º', 'ðŸ˜º', 'ðŸ˜º@results', 'FOUND.', 'View', 'all', 'teams.', 'MAD', 'Prod', 'Fundraistrick.', '350', '10th', 'Ave,', 'Suite', '1100.', 'San', 'Diego,', 'CA', '92101', 'US.', 'Back', 'to', 'top.', 'Donor', 'Support', 'braistrick@stayclassy.org']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.tokenizer(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result result result result result run upper case run run view team mad prod fundraistrick ave suite san diego donor support braistrickorg', 'run bunda bunda bunda result result view team prod fundraistrick ave suite san diego donor support braistrickorg', '', 'result result view team prod fundraistrick ave suite san diego donor support braistrickorg', 'bunda result result view team prod fundraistrick ave suite san diego donor support']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.clean_text(list_of_strings, lemmatizer=lemmatize_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.fit(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 3, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.transform(list_of_strings).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ave  braistrickorg  bunda  case  diego  donor  fundraistrick  mad  prod  \\\n",
       "0    1              1      0     1      1      1              1    1     1   \n",
       "1    1              1      3     0      1      1              1    0     1   \n",
       "2    0              0      0     0      0      0              0    0     0   \n",
       "3    1              1      0     0      1      1              1    0     1   \n",
       "4    1              0      1     0      1      1              1    0     1   \n",
       "\n",
       "   result  run  san  suite  support  team  upper  view  \n",
       "0       5    3    1      1        1     1      1     1  \n",
       "1       2    1    1      1        1     1      0     1  \n",
       "2       0    0    0      0        0     0      0     0  \n",
       "3       2    0    1      1        1     1      0     1  \n",
       "4       2    0    1      1        1     1      0     1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.bow_table(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.save_pipe('test')\n",
    "# nlp.load_pipe('test.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vectorize using TF-IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.194725Z",
     "start_time": "2018-08-12T19:36:54.190607Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp2 = nlp_preprocessor(lemmatizer=lemmatize_list, vectorizer=TfidfVectorizer(lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.214380Z",
     "start_time": "2018-08-12T19:36:54.197369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ave', 'braistrickorg', 'bunda', 'case', 'diego', 'donor', 'fundraistrick', 'mad', 'prod', 'result', 'run', 'san', 'suite', 'support', 'team', 'upper', 'view']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.148219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.623434</td>\n",
       "      <td>0.535675</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.198340</td>\n",
       "      <td>0.716815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.509431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.499209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ave  braistrickorg     bunda      case     diego     donor  \\\n",
       "0  0.124687       0.148219  0.000000  0.221318  0.124687  0.124687   \n",
       "1  0.166850       0.198340  0.716815  0.000000  0.166850  0.166850   \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.254715       0.302789  0.000000  0.000000  0.254715  0.254715   \n",
       "4  0.249604       0.000000  0.357447  0.000000  0.249604  0.249604   \n",
       "\n",
       "   fundraistrick       mad      prod    result       run       san     suite  \\\n",
       "0       0.124687  0.221318  0.124687  0.623434  0.535675  0.124687  0.124687   \n",
       "1       0.166850  0.000000  0.166850  0.333700  0.238938  0.166850  0.166850   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3       0.254715  0.000000  0.254715  0.509431  0.000000  0.254715  0.254715   \n",
       "4       0.249604  0.000000  0.249604  0.499209  0.000000  0.249604  0.249604   \n",
       "\n",
       "    support      team     upper      view  \n",
       "0  0.124687  0.124687  0.221318  0.124687  \n",
       "1  0.166850  0.166850  0.000000  0.166850  \n",
       "2  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.254715  0.254715  0.000000  0.254715  \n",
       "4  0.249604  0.249604  0.000000  0.249604  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.fit(list_of_strings)\n",
    "print(nlp2.vectorizer.get_feature_names())\n",
    "nlp2.bow_table(list_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:56.475435Z",
     "start_time": "2018-08-12T19:36:54.216987Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "ng_train_data = ng_train.data\n",
    "ng_train_targets = ng_train.target\n",
    "\n",
    "ng_test = datasets.fetch_20newsgroups(subset='test', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "\n",
    "ng_test_data = ng_test.data\n",
    "ng_test_targets = ng_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp with stemmer\n",
    "nlp = nlp_preprocessor()\n",
    "\n",
    "# nlp lemmatizing\n",
    "nlp1 = nlp_preprocessor(lemmatizer=lemmatize_list)\n",
    "\n",
    "# nlp with CountVectorizer vectorizer\n",
    "nlp2 = nlp_preprocessor(vectorizer=CountVectorizer(lowercase=False))\n",
    "\n",
    "# nlp with TfidfVectorizer\n",
    "nlp3 = nlp_preprocessor(vectorizer=TfidfVectorizer(lowercase=False))\n",
    "\n",
    "# nlp with lemmatizer_list function and TfidfVectorizer\n",
    "nlp4 = nlp_preprocessor(lemmatizer=lemmatize_list, vectorizer=TfidfVectorizer(lowercase=False))\n",
    "\n",
    "nlp_chains = [nlp, nlp1, nlp2, nlp3, nlp4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:10.328227Z",
     "start_time": "2018-08-12T19:36:56.477401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 0: 0.9158371040723982\n",
      "15.898925065994263\n",
      "Chain 1: 0.9194570135746606\n",
      "22.879443883895874\n",
      "Chain 2: 0.9158371040723982\n",
      "16.186060190200806\n",
      "Chain 3: 0.9067873303167421\n",
      "17.62559413909912\n",
      "Chain 4: 0.9076923076923077\n",
      "31.655200958251953\n",
      "104.24631810188293\n"
     ]
    }
   ],
   "source": [
    "# iterate over the nlp instantiated classes\n",
    "\n",
    "import time\n",
    "\n",
    "start_start = time.time()\n",
    "\n",
    "for ix, chain in enumerate(nlp_chains):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # model to be used\n",
    "    nb = MultinomialNB()\n",
    "    \n",
    "    # fit the preprocessed data\n",
    "    chain.fit(ng_train_data)\n",
    "    \n",
    "    # transform the train dataset\n",
    "    train_data = chain.transform(ng_train_data)\n",
    "    \n",
    "    # transform the test dataset\n",
    "    test_data = chain.transform(ng_test_data)\n",
    "    \n",
    "    # fit the model\n",
    "    nb.fit(train_data, ng_train_targets)\n",
    "    \n",
    "    # get the accuracy\n",
    "    accuracy = nb.score(test_data, ng_test_targets)\n",
    "    \n",
    "    # print the results\n",
    "    print(\"Chain {}: {}\".format(ix, accuracy))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "final_end = time.time()\n",
    "print(final_end - start_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised: Classification\n",
    "\n",
    "Here we'll write a class to predict a class given the text of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:22.978597Z",
     "start_time": "2018-08-12T19:37:10.343254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9158371040723982"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = nlp_preprocessor()\n",
    "nlp_pipe = supervised_nlp(MultinomialNB(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.730306Z",
     "start_time": "2018-08-12T19:37:22.981131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615384615384616"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp_pipe = supervised_nlp(LinearSVC(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised: Topic Modeling\n",
    "\n",
    "We don't want to make a prediction with this example, simply to find topics and have the ability to cast our data into the \"topic space\" from the \"word space.\" With this in mind, we'll add a transform feature and also the ability to print out the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.760623Z",
     "start_time": "2018-08-12T19:37:35.743417Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "\n",
    "cleaning_pipe = nlp_preprocessor(vectorizer=cv)\n",
    "topic_chain = topic_modeling_nlp(TruncatedSVD(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.687204Z",
     "start_time": "2018-08-12T19:37:35.762876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1661, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.734831Z",
     "start_time": "2018-08-12T19:37:36.689485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0jpeg image file images gif format files data software version\n",
      "Topic #1jpeg gif file quality jfif viewer quicktime programs colors quantization\n",
      "Topic #2jesus god atheists people matthew atheism religious gd religion prophecy\n",
      "Topic #3jesus matthew gd prophecy psalm messiah isaiah lord israel prophet\n",
      "Topic #4send ray graphics mail objects rayshade file stuff files format\n",
      "Topic #5argument fallacy conclusion true argumentum premises false valid inference form\n",
      "Topic #6data ftp model grass vertex sgi pci ibm contact jpeg\n",
      "Topic #7posting god response subject typical universe evidence einstein formal watchmaker\n",
      "Topic #8game year runs good hit team dont cubs win run\n",
      "Topic #9den px pz double radius sqrtden pxpxpypypzpz theta pole rtheta\n",
      "Topic #10compass opcols int oprows compassop row inputimage rowcol oprowscol char\n",
      "Topic #11program bits read menu display change pressing files file dont\n",
      "Topic #12cubs suck game atheists lost runs program file jesus bits\n",
      "Topic #13cubs suck people dont point islam evidence life otis values\n",
      "Topic #14lost idle san york chicago sox american national louis angeles\n"
     ]
    }
   ],
   "source": [
    "topic_chain.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.741910Z",
     "start_time": "2018-08-12T19:37:36.737246Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_chain = topic_modeling_nlp(LatentDirichletAllocation(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:42.475762Z",
     "start_time": "2018-08-12T19:37:36.745876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0plot things graphics user wip concept tiff hips control dont\n",
      "Topic #1god exist dont gods point existence ive people doesnt faith\n",
      "Topic #2dont people law email question ive card years wrong innocent\n",
      "Topic #3send graphics mail ray files dont stuff objects rayshade faq\n",
      "Topic #4dont god good time people bible atheist religion book life\n",
      "Topic #5question dont den time answer problem px god pz men\n",
      "Topic #6lost york san phillies chicago louis west philadelphia reds cincinnati\n",
      "Topic #7computer graphics time years virtual reality systems dont university conference\n",
      "Topic #8jesus people things points dont time prophecy jewish compass gd\n",
      "Topic #9image data software package images graphics processing program version ftp\n",
      "Topic #10books vinge computer true description reality life gibson vernor dont\n",
      "Topic #11year team game good games runs players baseball hit win\n",
      "Topic #12god people atheists argument true atheism islam religious dont religion\n",
      "Topic #13jpeg file image files gif format images color program bit\n",
      "Topic #14cubs suck sex matthew edge dont people john mark game\n"
     ]
    }
   ],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape\n",
    "topic_chain.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
