{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheetcheats\n",
    "\n",
    "## Math symbols and sheets:\n",
    "\n",
    "symbols: https://www.rapidtables.com/math/symbols/Basic_Math_Symbols.html\n",
    "\n",
    "\n",
    "## Machine learning sheets\n",
    "https://github.com/kailashahirwar/cheatsheets-ai\n",
    "\n",
    "* Overview of neural networds: https://www.asimovinstitute.org/neural-network-zoo/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals:\n",
    "\n",
    "* Natural Language Process (NLP)\n",
    "* Natural Language Understanding (NLU)\n",
    "* Natural Language Generate (NLG)\n",
    "* Speech Recognition\n",
    "* Vistual Object Recognition\n",
    "\n",
    "\n",
    "# Tasks:\n",
    "* Intent Classification\n",
    "* Slot Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks:\n",
    "\n",
    "* Recurrent Neural Network (RNN)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Long Short-Term Memory (LSTM)\n",
    "* RNN-Language Model (RNNLM)\n",
    "* Feedforward Neural Network Language Model (NNLM)\n",
    "* RNN-Encoder: two recurrent neural networds: http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:\n",
    "\n",
    "* Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014). They have the ability to encode the source text into an internal fixed-length representation called context vector. \n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language representation model  (architectures):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "MODELS = [('BertModel',       'BertTokenizer',       'bert-base-uncased'),\n",
    "          ('OpenAIGPTModel',  'OpenAIGPTTokenizer',  'openai-gpt'),\n",
    "          ('GPT2Model',       'GPT2Tokenizer',       'gpt2'),\n",
    "          ('CTRLModel',       'CTRLTokenizer',       'ctrl'),\n",
    "          ('TransfoXLModel',  'TransfoXLTokenizer',  'transfo-xl-wt103'),\n",
    "          ('XLNetModel',      'XLNetTokenizer',      'xlnet-base-cased'),\n",
    "          ('XLMModel',        'XLMTokenizer',        'xlm-mlm-enfr-1024'),\n",
    "          ('DistilBertModel', 'DistilBertTokenizer', 'distilbert-base-cased'),\n",
    "          ('RobertaModel',    'RobertaTokenizer',    'roberta-base'),\n",
    "          ('XLMRobertaModel', 'XLMRobertaTokenizer', 'xlm-roberta-base'),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline:\n",
    "1. __Preprocessing__: encode labels, tokenize using BertTokenizer, split into train and test set\n",
    "2. __Modeling__: \n",
    "3. __Optimizing__:\n",
    "4. __Evaluating__: using F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:\n",
    "\n",
    "* Perpexity tests\n",
    "* BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts:\n",
    "\n",
    "* __Transformer__: one type of network built with attention. It applies attention mechanisms to gather information about the relevant context of a given world, and then encode that context in a rich vector that represent the word: ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch \n",
    "> * https://huggingface.co/transformers/\n",
    "> * https://github.com/huggingface/transformers\n",
    "\n",
    "* __Sequence-to-sequence__: A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items. https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "\n",
    "* __WordEmbedding__: https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "\n",
    "* __Softmax__: I think it is an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline and sources:\n",
    "\n",
    "2014: Use of Neural Machine Translation by applying neural network moels to learn a statistical model for machine translation.\n",
    "\n",
    "Sequence to Sequence Learning with Neural Networks: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "\n",
    "Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation: http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf\n",
    "\n",
    "Attention is All You Need: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "--- \n",
    "* Joint models (references to datasets): https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets:\n",
    "\n",
    "* WMT-14: English to French translation task\n",
    "* ATIS (TÃ¼r et al., 2010): include records of people making flight reservation\n",
    "> * Data source: https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk?select=atis.train.pkl\n",
    "\n",
    "* Snips (Coucke et al., 2018): snips of personal voice assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials:\n",
    "* A Comprehensive Guide to Build your own Language Model in Python!: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "\n",
    "* Hugging Face Releases New NLP â€˜Tokenizersâ€™ Library Version (v0.8.0): https://www.analyticsvidhya.com/blog/2020/06/hugging-face-tokenizers-nlp-library/\n",
    "\n",
    "* Neural Machine Translation (seq2seq) Tutorial: https://github.com/tensorflow/nmt\n",
    "\n",
    "* Coursera assignment: https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/P4CTb/lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
