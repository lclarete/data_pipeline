{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheetcheats\n",
    "\n",
    "## Math symbols and sheets:\n",
    "\n",
    "symbols: https://www.rapidtables.com/math/symbols/Basic_Math_Symbols.html\n",
    "\n",
    "\n",
    "## Machine learning sheets\n",
    "https://github.com/kailashahirwar/cheatsheets-ai\n",
    "\n",
    "* Overview of neural networds: https://www.asimovinstitute.org/neural-network-zoo/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals:\n",
    "\n",
    "* Natural Language Process (NLP)\n",
    "* Natural Language Understanding (NLU)\n",
    "* Natural Language Generate (NLG)\n",
    "* Speech Recognition\n",
    "* Vistual Object Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d\n",
    "\n",
    "* Summarizing word vectors: classic approach of Bag-of-words, used with one-hot word vectors. Basically, it summarizes a word vector by weighing schemes.\n",
    "\n",
    "\n",
    "* Topic modeling: LDA and PLSI. It generate a document embedding space meant to model and explain word distribution in the corpus and where dimentions can be seen as latent semantic structures hidden in the data, and are thus useful in our context.\n",
    "\n",
    "    * LDA: \n",
    "\n",
    "\n",
    "* Encoder-decoder models: also called probabilistic language models, cover models such as  doc2vec and skip-through. Gained momentum with applications to word embedding generation.\n",
    "\n",
    "\n",
    "* Supervised representation learning: \n",
    "\n",
    "\n",
    "* Unsupervised representation: quick-thought and Word Mover's Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tasks\n",
    "* text classification\n",
    "* topic modeling\n",
    "* sentimental analysis\n",
    "* text summarization\n",
    "\n",
    "## Tasks:\n",
    "* Intent Classification\n",
    "* Slot Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks:\n",
    "\n",
    "* Recurrent Neural Network (RNN)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Long Short-Term Memory (LSTM)\n",
    "* RNN-Language Model (RNNLM)\n",
    "* Feedforward Neural Network Language Model (NNLM)\n",
    "* RNN-Encoder: two recurrent neural networds: http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:\n",
    "\n",
    "\n",
    "* Sparse representation: Bag of Words, TF-IDF, N-Grams\n",
    "\n",
    "* Word Embedding: Word2Vec, Glove\n",
    "\n",
    "* Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014). They have the ability to encode the source text into an internal fixed-length representation called context vector. \n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "* GloVe - Global Vectors for Word Representation. The word embedding from Standford NLP.\n",
    "\n",
    "It is defined as \"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\"\n",
    "\n",
    "\n",
    "The smallest file is named \"Glove.6B.zip\". The size of the file is 822 MB. The file contains 50, 100, 200, and 300 dimensional word vectors for 400k words. We will be using the 100 dimensional vector.\n",
    "\n",
    "source: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Git: https://github.com/stanfordnlp/GloVe\n",
    "\n",
    "Glove Algorithm:\n",
    "http://text2vec.org/glove.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language representation model  (architectures):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "MODELS = [('BertModel',       'BertTokenizer',       'bert-base-uncased'),\n",
    "          ('OpenAIGPTModel',  'OpenAIGPTTokenizer',  'openai-gpt'),\n",
    "          ('GPT2Model',       'GPT2Tokenizer',       'gpt2'),\n",
    "          ('CTRLModel',       'CTRLTokenizer',       'ctrl'),\n",
    "          ('TransfoXLModel',  'TransfoXLTokenizer',  'transfo-xl-wt103'),\n",
    "          ('XLNetModel',      'XLNetTokenizer',      'xlnet-base-cased'),\n",
    "          ('XLMModel',        'XLMTokenizer',        'xlm-mlm-enfr-1024'),\n",
    "          ('DistilBertModel', 'DistilBertTokenizer', 'distilbert-base-cased'),\n",
    "          ('RobertaModel',    'RobertaTokenizer',    'roberta-base'),\n",
    "          ('XLMRobertaModel', 'XLMRobertaTokenizer', 'xlm-roberta-base'),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline:\n",
    "1. __Preprocessing__: encode labels, tokenize using BertTokenizer, split into train and test set\n",
    "2. __Modeling__: \n",
    "3. __Optimizing__:\n",
    "4. __Evaluating__: using F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:\n",
    "\n",
    "* Perpexity tests\n",
    "* BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts:\n",
    "\n",
    "* __Transformer__: one type of network built with attention. It applies attention mechanisms to gather information about the relevant context of a given world, and then encode that context in a rich vector that represent the word: ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch \n",
    "> * https://huggingface.co/transformers/\n",
    "> * https://github.com/huggingface/transformers\n",
    "\n",
    "* __Sequence-to-sequence__: A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items. https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "\n",
    "* __WordEmbedding__: https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "\n",
    "* __Softmax__: I think it is an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline and sources:\n",
    "\n",
    "2014: Use of Neural Machine Translation by applying neural network moels to learn a statistical model for machine translation.\n",
    "\n",
    "Sequence to Sequence Learning with Neural Networks: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "\n",
    "Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation: http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf\n",
    "\n",
    "Attention is All You Need: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "--- \n",
    "* Joint models (references to datasets): https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets:\n",
    "\n",
    "* WMT-14: English to French translation task\n",
    "* ATIS (TÃ¼r et al., 2010): include records of people making flight reservation\n",
    "> * Data source: https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk?select=atis.train.pkl\n",
    "\n",
    "* Snips (Coucke et al., 2018): snips of personal voice assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials:\n",
    "* A Comprehensive Guide to Build your own Language Model in Python!: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "\n",
    "* Hugging Face Releases New NLP â€˜Tokenizersâ€™ Library Version (v0.8.0): https://www.analyticsvidhya.com/blog/2020/06/hugging-face-tokenizers-nlp-library/\n",
    "\n",
    "* Neural Machine Translation (seq2seq) Tutorial: https://github.com/tensorflow/nmt\n",
    "\n",
    "* Coursera assignment: https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/P4CTb/lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
