{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Diagnosis -> Clean_1_regex -> Normalize -> Clean_2_sw -> Create_Vocab -> Clean_Vocab -> Vectorize -> Modeling\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "__Challenges:__\n",
    "* Include new functions in the preprocessing_custom file\n",
    "* Follow coursera course\n",
    "* Thems based on regular expressions -- I already have a function that does it -- refactoring it\n",
    "_____\n",
    "\n",
    "__Semantic challenges: Look for regex patterns:__ \n",
    "* Is that possible to search for word patterns using regular expressions? Such as upper case words?\n",
    "* Include new features not only based on regex, such as: number of words, len of words, n characters, len characters, etc.\n",
    "\n",
    "\n",
    "____ \n",
    "\n",
    "__Tutorials__:\n",
    "\n",
    "* Text processing pipeline: https://www.tutorialspoint.com/python_text_processing/python_text_processing_environment.htm\n",
    "\n",
    "* AI Pipeline: https://www.tutorialspoint.com/artificial_intelligence_with_python/index.htm\n",
    "\n",
    "\n",
    "Regex material:\n",
    "* https://towardsdatascience.com/nlp-basics-understanding-regular-expressions-fc7c7746bc70#:~:text=Simply%20put%2C%20a%20regular%20expression,specify%20a%20disjunction%20of%20characters.\n",
    "\n",
    "* https://web.stanford.edu/~jurafsky/slp3/2.pdf\n",
    "* https://www.regular-expressions.info/replacecase.html#:~:text=In%20the%20regex%20%5CU%5Cw,is%20not%20a%20word%20character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic\n",
    "Test strings and regex dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "## include string\n",
    "## include list of strings\n",
    "## include regex dict\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "s = 'result results resulted resulting run UPPER CASE @you running ran No #results  ðŸ˜º ðŸ˜º ðŸ˜º@results FOUND. View all teams. MAD Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org'\n",
    "s1 = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb https://www.w3schools.com/python/python_regex.asp'\n",
    "s2 = 'https://www.w3schools.com/python/python_regex.asp No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s3 = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s4 = 'results results found view teams prod fundraistrick ave suite san diego back top donor support'\n",
    "list_of_strings = [s, s1, s2, s3, s4]\n",
    "\n",
    "dict_regex = {\n",
    "    'hashtags': r'#(\\w+)',\n",
    "    # returns not only mentions, but\n",
    "    # part of the email after the @\n",
    "    'mentions': r'@(\\w+)',\n",
    "    'emails': r'',\n",
    "    'links': r'https?:\\/\\/.*[\\r\\n]*',\n",
    "    'remove_RT': '^RT[\\s]+',\n",
    "    'numbers': r'\\d+',\n",
    "    'symbols': r'',\n",
    "    'punctionation2': '[^\\w\\s]',\n",
    "    'punctionation': '[%s]' % re.escape(string.punctuation),\n",
    "    'periods': '\\.',\n",
    "    'exclamation points': '\\!',\n",
    "    'question marks': '\\?',\n",
    "    'upper case words': '[A-Z][A-Z\\d]+',\n",
    "    # https://stackoverflow.com/questions/39536390/match-unicode-emoji-in-python-regex\n",
    "    'emojis': '\\d+(.*?)[\\u263a-\\U0001f645]',\n",
    "    'emojis_work': \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\",\n",
    "    'upper case': '[A-Z][A-Z\\d]+'\n",
    "}\n",
    "\n",
    "list_of_regex_values = list(dict_regex.values())\n",
    "list_of_regex_keys = list(dict_regex.keys())\n",
    "\n",
    "list_regex = ['ðŸ˜º', 'ðŸ˜º ðŸ˜º', 'ðŸ˜º ðŸ˜º ðŸ˜º', 'prod', 'results', 'suite', ' ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_pipeline.pre_processing_text.s_string_regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b37a0f49ad21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_processing_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_string_regex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre_match_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_processing_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_string_regex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre_count_match_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_processing_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_string_regex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre_sub_match_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_pipeline.pre_processing_text.s_string_regex'"
     ]
    }
   ],
   "source": [
    "from data_pipeline.pre_processing_text.s_string_regex import re_match_string\n",
    "from data_pipeline.pre_processing_text.s_string_regex import re_count_match_string\n",
    "from data_pipeline.pre_processing_text.s_string_regex import re_sub_match_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_match_string(s, list_of_regex_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_count_match_string(s, list_of_regex_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sub_match_string(s, list_of_regex_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.clean_list_regex import func_re_list_strings\n",
    "from data_pipeline.pre_processing_text.clean_list_regex import n_re_matches_list_strings\n",
    "from data_pipeline.pre_processing_text.clean_list_regex import list_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_re_list_strings(re_count_match_string, \n",
    "                     list_of_strings, \n",
    "                     list_of_regex_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through a list of regex over a list of strings\n",
    "t = n_re_matches_list_strings(\n",
    "    re_match_string,\n",
    "    list_of_strings,\n",
    "    dict_regex\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_df(t, list_of_regex_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnoses: count and return the occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.diagnosis_regex import df_count_regex_matches\n",
    "from data_pipeline.pre_processing_text.diagnosis_regex import df_matches\n",
    "\n",
    "from data_pipeline.pre_processing_text.clean_list_regex import sub_list_strings_list_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of regex\n",
    "df_count_regex_matches(list_of_strings, dict_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the identified patterns\n",
    "df_matches(list_of_strings, dict_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of strings, based on a list of regex\n",
    "list_cleaned = sub_list_strings_list_regex(list_of_strings, list_of_regex_values)\n",
    "list_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(list_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_clean = list_cleaned[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.norm_lemmatize import lemmatize_list, lemmatize_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lemma = lemmatize_string(s_clean)\n",
    "s_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lemmatized = list(map(lemmatize_string, list_cleaned))\n",
    "list_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lemmas = lemmatize_list(list_cleaned)\n",
    "list_of_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.apply(lemmatize_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.clean_list_stopwords import remove_stopwords_list, remove_stopwords_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['fundraistrick', 'all', 'to', 'th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_string(s_lemma, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lists_sw = remove_stopwords_list(list_of_lemmas, stopwords)\n",
    "clean_lists_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.preprocess_string_list import preprocessing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_list = preprocessing_list(list_of_strings, list_of_regex_values, stopwords)\n",
    "preprocessed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.spacy_preprocessing import get_hotwords_list\n",
    "from data_pipeline.pre_processing_text.spacy_preprocessing import spacy_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temptative 1\n",
    "cleaned_spacy_list = get_hotwords_list(list_of_strings)\n",
    "cleaned_spacy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temptative 2\n",
    "hot_words_spacy_list = spacy_tokenize_list(list_of_strings)\n",
    "hot_words_spacy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.preprocess_string_list import preprocessing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_list(list_of_strings, list_of_regex_values, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.vocab_create_clean import create_vocab\n",
    "from data_pipeline.pre_processing_text.vocab_create_clean import drop_rare_words\n",
    "from data_pipeline.pre_processing_text.vocab_create_clean import drop_common_words\n",
    "from data_pipeline.pre_processing_text.vocab_create_clean import count_words_list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(clean_lists_sw)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words_list_of_strings(clean_lists_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rare_words(clean_lists_sw, n_count_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_common_words(clean_lists_sw, n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.plot_viz.my_wordcloud import wordcloud_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_figure(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.modeling.my_topicmodeling import lda_topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model, doc_term_matrix, dictionary = lda_topic_model(list_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
