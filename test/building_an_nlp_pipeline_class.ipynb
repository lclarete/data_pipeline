{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a class to manage our NLP pipelines\n",
    "* https://www.youtube.com/watch?v=SG6jdlBx_vQ\n",
    "    \n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/tree/master/nlp_pipeline_manager\n",
    "\n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/blob/master/nlp_pipeline_manager/pipeline_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's such a pain to manage all the permutations of NLP cleaners/tokenizers/vectorizers/stemmers/etc, we're going to build a class that takes all of those pieces in and manages the pipelines for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'resulting result results resulted resulting run UPPER CASE @you running ran No #results  ðŸ˜º ðŸ˜º ðŸ˜º@results FOUND. View all teams. MAD Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org'\n",
    "s1 = 'run bunda bunda bunda No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb https://www.w3schools.com/python/python_regex.asp'\n",
    "s2 = 'https://www.w3schools.com/python/python_regex.asp No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s3 = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s4 = 'bunda results results found view teams prod fundraistrick ave suite san diego back top donor support'\n",
    "s_pt = 'vou. vindo vamos. testar? uma nova bibliotecas para portuguÃªs eu vou tu vai ele foi correr corriam corrida'\n",
    "list_of_strings = [s, s1, s2, s3, s4]\n",
    "corpus = list_of_strings\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "dict_regex = {\n",
    "    'hashtags': r'#(\\w+)',\n",
    "    # returns not only mentions, but\n",
    "    # part of the email after the @\n",
    "    'mentions': r'@(\\w+)',\n",
    "    'emails': r'',\n",
    "    'links': r'https?:\\/\\/.*[\\r\\n]*',\n",
    "    'remove_RT': r'^RT[\\s]+',\n",
    "    'numbers': r'\\d+',\n",
    "    'symbols': r'',\n",
    "    'punctionation2': r'[^\\w\\s]',\n",
    "    'punctionation': r'[%s]' % re.escape(string.punctuation),\n",
    "    'periods': r'\\.',\n",
    "    'exclamation points': r'\\!',\n",
    "    'question marks': r'\\?',\n",
    "    'upper case words': r'[A-Z][A-Z\\d]+',\n",
    "    # https://stackoverflow.com/questions/39536390/match-unicode-emoji-in-python-regex\n",
    "    'emojis': r'\\d+(.*?)[\\u263a-\\U0001f645]',\n",
    "    'emojis_work': r\"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\",\n",
    "    'upper case': r'[A-Z][A-Z\\d]+'\n",
    "}\n",
    "\n",
    "regex_emojis = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "list_of_regex_values = list(dict_regex.values())\n",
    "list_of_regex_keys = list(dict_regex.keys())\n",
    "\n",
    "sw = ['ðŸ˜º', 'ðŸ˜º ðŸ˜º', 'ðŸ˜º ðŸ˜º ðŸ˜º', 'prod', 'suite', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:53.819932Z",
     "start_time": "2018-08-12T19:36:52.882108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.6 (default, Jan  7 2020, 16:28:00) \n",
      "[Clang 11.0.0 (clang-1100.0.33.8)] \n",
      "\n",
      "Matplotlib Version: 3.2.0\n",
      "Numpy Version: 1.18.1\n",
      "Pandas Version: 0.25.3\n",
      "NLTK Version: 3.5\n",
      "sklearn Version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "# arrays and tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "# viz\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numbers and calculation\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "# files\n",
    "import sys\n",
    "\n",
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('NLTK', nltk), ('sklearn',sklearn))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pipeline: preprocessing and supervised nlp\n",
    "from data_pipeline.pre_processing_text.class_preprocessing import nlp_preprocessor\n",
    "from data_pipeline.modeling.class_supervised_ml import supervised_nlp\n",
    "from data_pipeline.modeling.my_topicmodeling import topic_modeling_nlp\n",
    "# from data_pipeline.pre_processing_text.norm_lemmatize import portuguese_stemmer\n",
    "\n",
    "# lemmatize and stem words\n",
    "from data_pipeline.pre_processing_text.norm_lemmatize import lemmatize_list\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "nlp = nlp_preprocessor(lemmatizer=lemmatize_list)\n",
    "# nlp = nlp_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resulting', 'result', 'results', 'resulted', 'resulting', 'run', 'UPPER', 'CASE', '@you', 'running', 'ran', 'No', '#results', '', 'ðŸ˜º', 'ðŸ˜º', 'ðŸ˜º@results', 'FOUND.', 'View', 'all', 'teams.', 'MAD', 'Prod', 'Fundraistrick.', '350', '10th', 'Ave,', 'Suite', '1100.', 'San', 'Diego,', 'CA', '92101', 'US.', 'Back', 'to', 'top.', 'Donor', 'Support', 'braistrick@stayclassy.org']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.tokenizer(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result result result result result run upper case run run view team mad prod fundraistrick ave suite san diego donor support braistrickorg', 'run bunda bunda bunda result result view team prod fundraistrick ave suite san diego donor support braistrickorg', '', 'result result view team prod fundraistrick ave suite san diego donor support braistrickorg', 'bunda result result view team prod fundraistrick ave suite san diego donor support']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.clean_text(list_of_strings, lemmatizer=lemmatize_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.fit(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 3, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.transform(list_of_strings).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ave  braistrickorg  bunda  case  diego  donor  fundraistrick  mad  prod  \\\n",
       "0    1              1      0     1      1      1              1    1     1   \n",
       "1    1              1      3     0      1      1              1    0     1   \n",
       "2    0              0      0     0      0      0              0    0     0   \n",
       "3    1              1      0     0      1      1              1    0     1   \n",
       "4    1              0      1     0      1      1              1    0     1   \n",
       "\n",
       "   result  run  san  suite  support  team  upper  view  \n",
       "0       5    3    1      1        1     1      1     1  \n",
       "1       2    1    1      1        1     1      0     1  \n",
       "2       0    0    0      0        0     0      0     0  \n",
       "3       2    0    1      1        1     1      0     1  \n",
       "4       2    0    1      1        1     1      0     1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.bow_table(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.save_pipe('test')\n",
    "# nlp.load_pipe('test.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vectorize using TF-IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.194725Z",
     "start_time": "2018-08-12T19:36:54.190607Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp2 = nlp_preprocessor(lemmatizer=lemmatize_list, vectorizer=TfidfVectorizer(lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.214380Z",
     "start_time": "2018-08-12T19:36:54.197369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ave', 'braistrickorg', 'bunda', 'case', 'diego', 'donor', 'fundraistrick', 'mad', 'prod', 'result', 'run', 'san', 'suite', 'support', 'team', 'upper', 'view']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.148219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.623434</td>\n",
       "      <td>0.535675</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.198340</td>\n",
       "      <td>0.716815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.166850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.509431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.499209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ave  braistrickorg     bunda      case     diego     donor  \\\n",
       "0  0.124687       0.148219  0.000000  0.221318  0.124687  0.124687   \n",
       "1  0.166850       0.198340  0.716815  0.000000  0.166850  0.166850   \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.254715       0.302789  0.000000  0.000000  0.254715  0.254715   \n",
       "4  0.249604       0.000000  0.357447  0.000000  0.249604  0.249604   \n",
       "\n",
       "   fundraistrick       mad      prod    result       run       san     suite  \\\n",
       "0       0.124687  0.221318  0.124687  0.623434  0.535675  0.124687  0.124687   \n",
       "1       0.166850  0.000000  0.166850  0.333700  0.238938  0.166850  0.166850   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3       0.254715  0.000000  0.254715  0.509431  0.000000  0.254715  0.254715   \n",
       "4       0.249604  0.000000  0.249604  0.499209  0.000000  0.249604  0.249604   \n",
       "\n",
       "    support      team     upper      view  \n",
       "0  0.124687  0.124687  0.221318  0.124687  \n",
       "1  0.166850  0.166850  0.000000  0.166850  \n",
       "2  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.254715  0.254715  0.000000  0.254715  \n",
       "4  0.249604  0.249604  0.000000  0.249604  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.fit(list_of_strings)\n",
    "print(nlp2.vectorizer.get_feature_names())\n",
    "nlp2.bow_table(list_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:56.475435Z",
     "start_time": "2018-08-12T19:36:54.216987Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "ng_train_data = ng_train.data\n",
    "ng_train_targets = ng_train.target\n",
    "\n",
    "ng_test = datasets.fetch_20newsgroups(subset='test', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "\n",
    "ng_test_data = ng_test.data\n",
    "ng_test_targets = ng_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp with stemmer\n",
    "nlp = nlp_preprocessor()\n",
    "\n",
    "# nlp lemmatizing\n",
    "nlp1 = nlp_preprocessor(lemmatizer=lemmatize_list)\n",
    "\n",
    "# nlp with CountVectorizer vectorizer\n",
    "nlp2 = nlp_preprocessor(vectorizer=CountVectorizer(lowercase=False))\n",
    "\n",
    "# nlp with TfidfVectorizer\n",
    "nlp3 = nlp_preprocessor(vectorizer=TfidfVectorizer(lowercase=False))\n",
    "\n",
    "# nlp with lemmatizer_list function and TfidfVectorizer\n",
    "nlp4 = nlp_preprocessor(lemmatizer=lemmatize_list, vectorizer=TfidfVectorizer(lowercase=False))\n",
    "\n",
    "nlp_chains = [nlp, nlp1, nlp2, nlp3, nlp4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:10.328227Z",
     "start_time": "2018-08-12T19:36:56.477401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 0: 0.9158371040723982\n",
      "20.290612936019897\n",
      "Chain 1: 0.9194570135746606\n",
      "54.94128394126892\n",
      "Chain 2: 0.9158371040723982\n",
      "30.068961143493652\n",
      "Chain 3: 0.9067873303167421\n",
      "30.367820978164673\n",
      "Chain 4: 0.9076923076923077\n",
      "27.94713592529297\n",
      "163.61649703979492\n"
     ]
    }
   ],
   "source": [
    "# iterate over the nlp instantiated classes\n",
    "\n",
    "import time\n",
    "\n",
    "start_start = time.time()\n",
    "\n",
    "for ix, chain in enumerate(nlp_chains):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # model to be used\n",
    "    nb = MultinomialNB()\n",
    "    \n",
    "    # fit the preprocessed data\n",
    "    chain.fit(ng_train_data)\n",
    "    \n",
    "    # transform the train dataset\n",
    "    train_data = chain.transform(ng_train_data)\n",
    "    \n",
    "    # transform the test dataset\n",
    "    test_data = chain.transform(ng_test_data)\n",
    "    \n",
    "    # fit the model\n",
    "    nb.fit(train_data, ng_train_targets)\n",
    "    \n",
    "    # get the accuracy\n",
    "    accuracy = nb.score(test_data, ng_test_targets)\n",
    "    \n",
    "    # print the results\n",
    "    print(\"Chain {}: {}\".format(ix, accuracy))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "final_end = time.time()\n",
    "print(final_end - start_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised: Classification\n",
    "\n",
    "Here we'll write a class to predict a class given the text of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:22.978597Z",
     "start_time": "2018-08-12T19:37:10.343254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9158371040723982"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = nlp_preprocessor()\n",
    "nlp_pipe = supervised_nlp(MultinomialNB(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.730306Z",
     "start_time": "2018-08-12T19:37:22.981131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615384615384616"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp_pipe = supervised_nlp(LinearSVC(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised: Topic Modeling\n",
    "\n",
    "We don't want to make a prediction with this example, simply to find topics and have the ability to cast our data into the \"topic space\" from the \"word space.\" With this in mind, we'll add a transform feature and also the ability to print out the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.760623Z",
     "start_time": "2018-08-12T19:37:35.743417Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "\n",
    "cleaning_pipe = nlp_preprocessor(vectorizer=cv)\n",
    "topic_chain = topic_modeling_nlp(TruncatedSVD(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.687204Z",
     "start_time": "2018-08-12T19:37:35.762876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1661, 15)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.734831Z",
     "start_time": "2018-08-12T19:37:36.689485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0jpeg image file images gif format files data software version\n",
      "Topic #1jpeg gif file quality jfif viewer quicktime programs colors quantization\n",
      "Topic #2jesus god atheists people matthew atheism religious gd religion prophecy\n",
      "Topic #3jesus matthew gd prophecy psalm messiah isaiah lord israel prophet\n",
      "Topic #4send ray graphics mail objects rayshade file stuff files format\n",
      "Topic #5argument fallacy conclusion true argumentum premises false valid inference form\n",
      "Topic #6data ftp model grass vertex sgi pci ibm contact jpeg\n",
      "Topic #7posting god response subject typical universe evidence einstein formal watchmaker\n",
      "Topic #8game year runs good hit team dont cubs win run\n",
      "Topic #9den px pz double radius pxpxpypypzpz sqrtden theta pole rtheta\n",
      "Topic #10compass opcols int oprows compassop row inputimage rowcol oprowscol char\n",
      "Topic #11program bits read menu display change pressing files file dont\n",
      "Topic #12cubs suck game atheists lost runs program file jesus bits\n",
      "Topic #13cubs suck people dont islam point evidence life otis book\n",
      "Topic #14lost idle san york chicago sox american national louis angeles\n"
     ]
    }
   ],
   "source": [
    "topic_chain.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.741910Z",
     "start_time": "2018-08-12T19:37:36.737246Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_chain = topic_modeling_nlp(LatentDirichletAllocation(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:42.475762Z",
     "start_time": "2018-08-12T19:37:36.745876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0year good players years dont league average baseball game lot\n",
      "Topic #1image program data display processing video color images mode card\n",
      "Topic #2jpeg image file gif files images format good bit color\n",
      "Topic #3game games fan phillies philadelphia team era wip time baseball\n",
      "Topic #4graphics data image package email ftp send software code file\n",
      "Topic #5time players sex bob software bronx dont good bobbeicotekcom beauchaine\n",
      "Topic #6dont women god people morris deletion men quran islamic games\n",
      "Topic #7fallacy conclusion graphics premises argumentum argument valid computer inference group\n",
      "Topic #8david reality win presentations runs virtual visualization scientific seminar robert\n",
      "Topic #9god people dont argument atheists true evidence religion islam exist\n",
      "Topic #10team year game runs lost hit games dont win braves\n",
      "Topic #11jesus people time matthew atheism god atheists bible prophecy religious\n",
      "Topic #12compass opcols int faq search graphics oprows data send dont\n",
      "Topic #13good dont cubs year suck years team time bit animals\n",
      "Topic #14polygon point program enviroleague polygons programs edge routine problem controller\n"
     ]
    }
   ],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape\n",
    "topic_chain.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
