{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## --- doc/ string method: cleaning --- ##########################\n",
    "\n",
    "# def sub_on_regex(string_text, regex):\n",
    "#     \"\"\"\n",
    "#     Read and compile a regex, \n",
    "#     substitute/ replace the pattern per nothing.\n",
    "    \n",
    "#     Support function to implement a list of regex.\n",
    "    \n",
    "#     Args:\n",
    "#         regex: regex pattern\n",
    "#         string_text: text type string\n",
    "#     Returns:\n",
    "#         new string without the string pattern and striped\n",
    "#     \"\"\"\n",
    "#     regex = re.compile(regex)\n",
    "#     return regex.sub('', string_text.lower()).strip()\n",
    "\n",
    "\n",
    "# def sub_regex_patterns(string_text, list_regex=list_re):\n",
    "#     \"\"\" \n",
    "#     Iterate through a list of regex patterns over a string\n",
    "#     Args:\n",
    "#         list_regex: list of regex patterns\n",
    "#         string_text: text type string\n",
    "#     Returns:\n",
    "#         new string with the replaced patterns\n",
    "#     \"\"\"    \n",
    "#     for regex in list_regex:\n",
    "#         # apply the sub_on_regex function\n",
    "#         string_text = sub_on_regex(string_text, regex)\n",
    "        \n",
    "#     return string_text\n",
    "\n",
    "# def remove_stopwords(string_text, stopwords=stopwords):\n",
    "#     \"\"\"Remove a list of words from a string\"\"\"\n",
    "#     return ' '.join([w for w in string_text.split() if w not in stopwords])\n",
    "\n",
    "\n",
    "def clean_string(string_text):\n",
    "    \"\"\"Apply  the regex cleaning methods and remove stopwords from a string\"\"\"\n",
    "    string_clean = sub_regex_patterns(string_text)\n",
    "    return remove_stopwords(string_clean)\n",
    "\n",
    "########################## --- list of strings cleaning methods --- ##########################\n",
    "\n",
    "def clean_list_of_string(list_of_strings):\n",
    "    \"\"\"Clean each string from a list of strings\"\"\"\n",
    "    l_clean = []\n",
    "    \n",
    "    # iterate through a list of strings\n",
    "    for s in list_of_strings:\n",
    "        # clean each string\n",
    "        string_cleared = clean_string(s)\n",
    "        # append each cleaned string into a list\n",
    "        l_clean.append(string_cleared)\n",
    "        \n",
    "    # return a list of cleaned strings\n",
    "    return l_clean\n",
    "\n",
    "\n",
    "########################## --- string method: tokenize --- ##########################\n",
    "\n",
    "# def tokenize(string_text):\n",
    "#     \"\"\"\n",
    "#     Return a list of words splitted by space\n",
    "#     Args:\n",
    "#         string_text: text type string\n",
    "#     \"\"\"\n",
    "#     return string_text.split()\n",
    "\n",
    "\n",
    "########################## --- vocab methods: list of strings --- ##########################\n",
    "\n",
    "# def count_freq(string_text):\n",
    "#     \"\"\"\n",
    "#     Counts the frequency of elements in a list.\n",
    "#     Not so useful. Should use Counter from collections instead\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # tokenize a string\n",
    "#     l = tokenize(string_text)\n",
    "    \n",
    "#     # returns a dictionary counting the number of words\n",
    "#     return {i:l.count(i) for i in l}\n",
    "\n",
    "def count_words_list_of_strings(list_of_strings):\n",
    "    \"\"\"\n",
    "    Returns a dict summing the number of words from each of a list os strings.\n",
    "    Args:\n",
    "        list_of_strings: list of docs as a string type\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    # iterate through a list\n",
    "    for string in list_of_strings:\n",
    "        # tokenize each of the strings\n",
    "        t = tokenize(i)\n",
    "        # count the number of words\n",
    "        count = Counter(t)\n",
    "        # append the dictionaries into a list\n",
    "        l.append(dict(count))\n",
    "    \n",
    "    # return the sum of all dictionaries\n",
    "    return sum_dicts(l)\n",
    "\n",
    "\n",
    "def sum_dicts(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Sum the elements from a list of dictionaries. \n",
    "    Support function to count_words_list_of_strings.\n",
    "    Args:\n",
    "        list_of_dicts: list of dictionaries with the frequency of each word\n",
    "    \"\"\"\n",
    "    return dict(reduce(operator.add, map(Counter, list_of_dicts)))\n",
    "\n",
    "\n",
    "def exclude_rare_words(list_of_strings, n_count_words=2):\n",
    "    \"\"\"\n",
    "    Exclude rare words.\n",
    "    Agrs:\n",
    "        d: dictionary counting words\n",
    "        n_count_words: minimum number of frequency to consider\n",
    "    Returns:\n",
    "        String without words with a minium counts\n",
    "    \"\"\"\n",
    "    d = count_words_list_of_strings(list_of_strings)\n",
    "    \n",
    "    return ' '.join([k for k, v in d.items() if v > n_count_words])\n",
    "\n",
    "\n",
    "def exclude_common_words(list_of_strings, n_top_words=20):\n",
    "    \"\"\"\n",
    "    Returns a string without the n_top_words common words\n",
    "    Args:\n",
    "        list_of_strings: list of string\n",
    "        n_top_words: number of most comon words to be excluded\n",
    "    \"\"\"\n",
    "    # create a dictionary\n",
    "    d = count_words_list_of_strings(list_of_strings)\n",
    "    \n",
    "    # \n",
    "    df = pd.DataFrame(data=d.items(), columns=['words', 'count']).sort_values(by='count', ascending=False)[n_top_words:]\n",
    "    return ' '.join(df.words.to_list())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### --- regex patterns --- ###########\n",
    "\n",
    "# list all patterns that should be removed\n",
    "\n",
    "# hyperlinks\n",
    "hyperlinks_re = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "\n",
    "# emails\n",
    "email_re = ''\n",
    "\n",
    "# punctuation\n",
    "punctionation_re = '[%s]' % re.escape(string.punctuation)\n",
    "\n",
    "# numbers\n",
    "bad_symbols_re = r'[^0-9a-z #+_a]'\n",
    "\n",
    "# spaces\n",
    "space_re = r'[/(){}\\[\\]\\|@#,;]'\n",
    "\n",
    "list_re = [hyperlinks_re, space_re, bad_symbols_re, hashtags_re, punctionation_re, regex_re]\n",
    "\n",
    "########### --- stopwords --- ###########\n",
    "stopwords = ['no', 'all', 'to', 'us', 'ca']\n",
    "\n",
    "\n",
    "########### --- string --- ###########\n",
    "s = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb',\n",
       " 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb']"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results results found view teams prod fundraistrick ave suite san diego back top donor support braistrickstayclassyorg'"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with one string\n",
    "s_clean = sub_regex_patterns(s, list_re)\n",
    "s_clean = remove_stopwords(s_clean)\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no results results found view all teams prod fundraistrick   ave suite  san diego ca  us back to top donor support braistrickstayclassyorg',\n",
       " 'no results results found view all teams prod fundraistrick   ave suite  san diego ca  us back to top donor support braistrickstayclassyorg']"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with a list of string\n",
    "l = [s, s]\n",
    "l_clean = list(map(sub_regex_patterns, l))\n",
    "l_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dicts = [a, b, a, b]\n",
    "d = sum_dicts(list_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_rare_words(d, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
