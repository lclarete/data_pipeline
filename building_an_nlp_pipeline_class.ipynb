{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "* https://www.youtube.com/watch?v=SG6jdlBx_vQ\n",
    "    \n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/tree/master/nlp_pipeline_manager\n",
    "\n",
    "* https://github.com/ZWMiller/nlp_pipe_manager/blob/master/nlp_pipeline_manager/pipeline_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a class to manage our NLP pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's such a pain to manage all the permutations of NLP cleaners/tokenizers/vectorizers/stemmers/etc, we're going to build a class that takes all of those pieces in and manages the pipelines for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'resulting result results resulted resulting run UPPER CASE @you running ran No #results  ðŸ˜º ðŸ˜º ðŸ˜º@results FOUND. View all teams. MAD Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org'\n",
    "s1 = 'run bunda No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb https://www.w3schools.com/python/python_regex.asp'\n",
    "s2 = 'https://www.w3schools.com/python/python_regex.asp No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s3 = 'No results results found. View all teams. Prod Fundraistrick. 350 10th Ave, Suite 1100. San Diego, CA 92101 US. Back to top. Donor Support braistrick@stayclassy.org. http://localhost:8888/notebooks/nlp/cleaning_sandbox.ipynb'\n",
    "s4 = 'bunda results results found view teams prod fundraistrick ave suite san diego back top donor support'\n",
    "list_of_strings = [s, s1, s2, s3, s4]\n",
    "corpus = list_of_strings\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "dict_regex = {\n",
    "    'hashtags': r'#(\\w+)',\n",
    "    # returns not only mentions, but\n",
    "    # part of the email after the @\n",
    "    'mentions': r'@(\\w+)',\n",
    "    'emails': r'',\n",
    "    'links': r'https?:\\/\\/.*[\\r\\n]*',\n",
    "    'remove_RT': '^RT[\\s]+',\n",
    "    'numbers': r'\\d+',\n",
    "    'symbols': r'',\n",
    "    'punctionation2': '[^\\w\\s]',\n",
    "    'punctionation': '[%s]' % re.escape(string.punctuation),\n",
    "    'periods': '\\.',\n",
    "    'exclamation points': '\\!',\n",
    "    'question marks': '\\?',\n",
    "    'upper case words': '[A-Z][A-Z\\d]+',\n",
    "    # https://stackoverflow.com/questions/39536390/match-unicode-emoji-in-python-regex\n",
    "    'emojis': '\\d+(.*?)[\\u263a-\\U0001f645]',\n",
    "    'emojis_work': \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\",\n",
    "    'upper case': '[A-Z][A-Z\\d]+'\n",
    "}\n",
    "\n",
    "regex_emojis = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "list_of_regex_values = list(dict_regex.values())\n",
    "list_of_regex_keys = list(dict_regex.keys())\n",
    "\n",
    "sw = ['ðŸ˜º', 'ðŸ˜º ðŸ˜º', 'ðŸ˜º ðŸ˜º ðŸ˜º', 'prod', 'suite', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.pre_processing_text.preprocess_string_list import preprocessing_list, preprocessing_string\n",
    "from data_pipeline.pre_processing_text.norm_lemmatize import lemmatize_string, lemmatize_list\n",
    "from data_pipeline.pre_processing_text.split_ngrams import tokenize, tokenize_list\n",
    "from data_pipeline.pre_processing_text.clean_list_stopwords import remove_stopwords_list\n",
    "from data_pipeline.pre_processing_text.clean_list_regex import sub_list_strings_list_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:53.819932Z",
     "start_time": "2018-08-12T19:36:52.882108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.6 (default, Jan  7 2020, 16:28:00) \n",
      "[Clang 11.0.0 (clang-1100.0.33.8)] \n",
      "\n",
      "Matplotlib Version: 3.2.0\n",
      "Numpy Version: 1.18.1\n",
      "Pandas Version: 0.25.3\n",
      "NLTK Version: 3.5\n",
      "sklearn Version: 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('NLTK', nltk), ('sklearn',sklearn))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:53.986272Z",
     "start_time": "2018-08-12T19:36:53.822664Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a class to implement data pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "class nlp_preprocessor:\n",
    "        \n",
    "    def __init__(self, vectorizer=CountVectorizer(), \n",
    "                 tokenizer=None, \n",
    "                 cleaning_function=None, \n",
    "                 lemmatizer=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        A class for pipelineing data in NLP problems. \n",
    "        User provides a series of tools (functions), and the\n",
    "        class manages all the training, transforming and \n",
    "        modificaion of the text data.\n",
    "\n",
    "        Inputs:\n",
    "        vectorizer: model to use for text vectorization \n",
    "        tokenizer: tokenizer to use; default is split into spaces\n",
    "        cleaning_functions: how to clean the data\n",
    "        \"\"\"\n",
    "        \n",
    "        # if tokenizer is not defined,\n",
    "        if not tokenizer:\n",
    "            # use splitter_strings as default\n",
    "            tokenizer = self.splitter_string\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # if cleaning_function is not defined,\n",
    "        if not cleaning_function:\n",
    "            # use the function clean_text as default\n",
    "            # TO DO: drop regex dict and stopwords data out of the .py files\n",
    "            # TO DO: structure the files in a way I can read the regex and \n",
    "            # stopwords as arguments inside the function\n",
    "            cleaning_function = self.clean_text\n",
    "            \n",
    "            \n",
    "        # TO DO: test new cleaning functions\n",
    "        self.cleaning_function = cleaning_function\n",
    "            \n",
    "            \n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "                \n",
    "    \n",
    "    def splitter_string(self, string):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits a string on spaces naively\n",
    "        \"\"\"\n",
    "        return string.split(' ')\n",
    "\n",
    "\n",
    "    def clean_text(self, lemmatizer, list_of_strings):\n",
    "        \"\"\"\n",
    "        Clean regex patterns, lemmatize words and remove stopwords\n",
    "        Input:\n",
    "            lemmatizer: function to lemmatize words\n",
    "            list_of_strings: list of strings\n",
    "        Output:\n",
    "            list of cleaned strings\n",
    "        \"\"\"\n",
    "        # clean regex patterns\n",
    "        clean_text = sub_list_strings_list_regex(list_of_strings)\n",
    "        # lemmatize words\n",
    "        clean_text = lemmatizer(clean_text)\n",
    "        # remove stopwords\n",
    "        return remove_stopwords_list(clean_text)\n",
    "    \n",
    "    \n",
    "    # for some reason, I can't use self.lemmatizer\n",
    "    def fit(self, lemmatizer, list_of_strings):\n",
    "        \"\"\"\n",
    "        Cleans the data and then fits the vectorizer with\n",
    "        the user provided text\n",
    "        \"\"\"\n",
    "#         clean_text = self.cleaning_function(list_of_strings, self.tokenizer, self.lemmatizer)\n",
    "        preprocessed_text = self.cleaning_function(lemmatizer, list_of_strings)\n",
    "\n",
    "        self.vectorizer.fit(preprocessed_text)\n",
    "        self._is_fit = True\n",
    "    \n",
    "\n",
    "    def transform(self, lemmatizer, list_of_strings, return_clean_text=False):\n",
    "        \"\"\"\n",
    "        Cleans any provided data and then transforms data\n",
    "        into a vectorized format based on the fit function.\n",
    "        Returns vectorized form of the data.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "#         clean_text = self.cleaning_function(list_of_strings, self.tokenizer, self.lemmatizer)\n",
    "        clean_text = self.cleaning_function(lemmatizer, list_of_strings)\n",
    "\n",
    "        if return_clean_text:\n",
    "            return clean_text\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "\n",
    "    \n",
    "    def bow_table(self, lemmatizer, list_of_strings):\n",
    "        \"\"\"\n",
    "        Apply the transformer to format a table counting the number of words per document\n",
    "        Input:\n",
    "            lemmatizer\n",
    "            list_of_strings\n",
    "        Output:\n",
    "            table counting the number of words per document\n",
    "        \"\"\"\n",
    "        data = self.transform(lemmatizer, list_of_strings).toarray()\n",
    "        columns = self.vectorizer.get_feature_names()\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "                raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+'.mdl', 'wb'))\n",
    "\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's test the model with the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result result result result result run UPPER CASE @you run run No # result   ðŸ˜º ðŸ˜º ðŸ˜º @results FOUND . View all team . MAD Prod Fundraistrick . 350 10th Ave , Suite 1100 . San Diego , CA 92101 US . Back to top . Donor Support braistrick@stayclassy.org '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_string(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "nlp = nlp_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resulting', 'result', 'results', 'resulted', 'resulting', 'run', 'UPPER', 'CASE', '@you', 'running', 'ran', 'No', '#results', '', 'ðŸ˜º', 'ðŸ˜º', 'ðŸ˜º@results', 'FOUND.', 'View', 'all', 'teams.', 'MAD', 'Prod', 'Fundraistrick.', '350', '10th', 'Ave,', 'Suite', '1100.', 'San', 'Diego,', 'CA', '92101', 'US.', 'Back', 'to', 'top.', 'Donor', 'Support', 'braistrick@stayclassy.org']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.tokenizer(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nlp.clean_text(list_of_strings, tokenizer=tokenize, lemmatizer=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result result result result result run upper case run run view team mad prod fundraistrick ave suite san diego donor support braistrickorg', 'run bunda result result view team prod fundraistrick ave suite san diego donor support braistrickorg', '', 'result result view team prod fundraistrick ave suite san diego donor support braistrickorg', 'bunda result result view team prod fundraistrick ave suite san diego donor support']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.clean_text(lemmatize_list, list_of_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.fit(lemmatize_list, list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.transform(lemmatize_list, list_of_strings).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ave  braistrickorg  bunda  case  diego  donor  fundraistrick  mad  prod  \\\n",
       "0    1              1      0     1      1      1              1    1     1   \n",
       "1    1              1      1     0      1      1              1    0     1   \n",
       "2    0              0      0     0      0      0              0    0     0   \n",
       "3    1              1      0     0      1      1              1    0     1   \n",
       "4    1              0      1     0      1      1              1    0     1   \n",
       "\n",
       "   result  run  san  suite  support  team  upper  view  \n",
       "0       5    3    1      1        1     1      1     1  \n",
       "1       2    1    1      1        1     1      0     1  \n",
       "2       0    0    0      0        0     0      0     0  \n",
       "3       2    0    1      1        1     1      0     1  \n",
       "4       2    0    1      1        1     1      0     1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.bow_table(lemmatize_list, list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.save_pipe('test')\n",
    "# nlp.load_pipe('test.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about using TF-IDF instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.194725Z",
     "start_time": "2018-08-12T19:36:54.190607Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp3 = nlp_preprocessor(vectorizer=TfidfVectorizer(lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:54.214380Z",
     "start_time": "2018-08-12T19:36:54.197369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ave', 'braistrickorg', 'bunda', 'case', 'diego', 'donor', 'fundraistrick', 'mad', 'prod', 'result', 'run', 'san', 'suite', 'support', 'team', 'upper', 'view']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave</th>\n",
       "      <th>braistrickorg</th>\n",
       "      <th>bunda</th>\n",
       "      <th>case</th>\n",
       "      <th>diego</th>\n",
       "      <th>donor</th>\n",
       "      <th>fundraistrick</th>\n",
       "      <th>mad</th>\n",
       "      <th>prod</th>\n",
       "      <th>result</th>\n",
       "      <th>run</th>\n",
       "      <th>san</th>\n",
       "      <th>suite</th>\n",
       "      <th>support</th>\n",
       "      <th>team</th>\n",
       "      <th>upper</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.148219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.623434</td>\n",
       "      <td>0.535675</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.124687</td>\n",
       "      <td>0.221318</td>\n",
       "      <td>0.124687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.269094</td>\n",
       "      <td>0.324174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.452741</td>\n",
       "      <td>0.324174</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.226370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.509431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.499209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ave  braistrickorg     bunda      case     diego     donor  \\\n",
       "0  0.124687       0.148219  0.000000  0.221318  0.124687  0.124687   \n",
       "1  0.226370       0.269094  0.324174  0.000000  0.226370  0.226370   \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.254715       0.302789  0.000000  0.000000  0.254715  0.254715   \n",
       "4  0.249604       0.000000  0.357447  0.000000  0.249604  0.249604   \n",
       "\n",
       "   fundraistrick       mad      prod    result       run       san     suite  \\\n",
       "0       0.124687  0.221318  0.124687  0.623434  0.535675  0.124687  0.124687   \n",
       "1       0.226370  0.000000  0.226370  0.452741  0.324174  0.226370  0.226370   \n",
       "2       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3       0.254715  0.000000  0.254715  0.509431  0.000000  0.254715  0.254715   \n",
       "4       0.249604  0.000000  0.249604  0.499209  0.000000  0.249604  0.249604   \n",
       "\n",
       "    support      team     upper      view  \n",
       "0  0.124687  0.124687  0.221318  0.124687  \n",
       "1  0.226370  0.226370  0.000000  0.226370  \n",
       "2  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.254715  0.254715  0.000000  0.254715  \n",
       "4  0.249604  0.249604  0.000000  0.249604  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3.fit(lemmatize_list, corpus)\n",
    "print(nlp3.vectorizer.get_feature_names())\n",
    "nlp3.bow_table(lemmatize_list, list_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what? Let's use some real data to try some different modeling approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:36:56.475435Z",
     "start_time": "2018-08-12T19:36:54.216987Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "ng_train_data = ng_train.data\n",
    "ng_train_targets = ng_train.target\n",
    "\n",
    "ng_test = datasets.fetch_20newsgroups(subset='test', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))\n",
    "\n",
    "ng_test_data = ng_test.data\n",
    "ng_test_targets = ng_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:10.328227Z",
     "start_time": "2018-08-12T19:36:56.477401Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = nlp_preprocessor(stemmer=PorterStemmer())\n",
    "nlp2 = nlp_preprocessor(vectorizer=CountVectorizer(lowercase=False))\n",
    "nlp3 = nlp_preprocessor(cleaning_function=new_clean_text, vectorizer=TfidfVectorizer(lowercase=False))\n",
    "nlp_chains = [nlp, nlp2, nlp3]\n",
    "\n",
    "for ix, chain in enumerate(nlp_chains):\n",
    "    nb = MultinomialNB()\n",
    "    chain.fit(ng_train_data)\n",
    "    train_data = chain.transform(ng_train_data)\n",
    "    test_data = chain.transform(ng_test_data)\n",
    "    nb.fit(train_data, ng_train_targets)\n",
    "    accuracy = nb.score(test_data, ng_test_targets)\n",
    "    print(\"Chain {}: {}\".format(ix, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to sweep all of the preprocessing into a class where we can control the pieces and parts that go in, and can see what comes out. If we wanted to, we could even add a model into the class as well and put the whole pipe into a single class that manages all of our challenges. In this case, we've left it outside for demo purposes. This also saves all of the pieces together, so we can just pickle a class object and that will keep the whole structure of our models together - such as the vectorizer and the stemmer we used, as well as the cleaning routine, so we don't lose any of the pieces if we want to run it on new data later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a model to the mix\n",
    "\n",
    "Depending on the type of model we want to build, we'll need to wrap the preprocessing class a little bit differently for the specific case. For example, if we're doing supervised learning, we'll want a `predict` method. If we're doing topic modeling, we'll want a `transform` method. To make that happen, I'll show a few examples below that wrap around the preprocessing class to make the most of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised: Classification\n",
    "\n",
    "Here we'll write a class to predict a class given the text of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:10.340551Z",
     "start_time": "2018-08-12T19:37:10.330954Z"
    }
   },
   "outputs": [],
   "source": [
    "class supervised_nlp:\n",
    "    \n",
    "    def __init__(self, model, preprocessing_pipeline=None):\n",
    "        \"\"\"\n",
    "        A pipeline for doing supervised nlp. Expects a model and creates\n",
    "        a preprocessing pipeline if one isn't provided.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self._is_fit = False\n",
    "        if not preprocessing_pipeline:\n",
    "            self.preprocessor = nlp_preprocessor()\n",
    "        else:\n",
    "            self.preprocessor = preprocessing_pipeline\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the vectorizer and model together using the \n",
    "        users input training data.\n",
    "        \"\"\"\n",
    "        self.preprocessor.fit(X)\n",
    "        train_data = self.preprocessor.transform(X)\n",
    "        self.model.fit(train_data, y)\n",
    "        self._is_fit = True\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes a prediction on the data provided by the users using the \n",
    "        preprocessing pipeline and provided model.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        test_data = self.preprocessor.transform(X)\n",
    "        preds = self.model.predict(test_data)\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for the model after using the trained\n",
    "        preprocessing pipeline to prepare the data.\n",
    "        \"\"\"\n",
    "        test_data = self.preprocessor.transform(X)\n",
    "        return self.model.score(test_data, y)\n",
    "    \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+\".mdl\",'wb'))\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:22.978597Z",
     "start_time": "2018-08-12T19:37:10.343254Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_pipe = supervised_nlp(MultinomialNB(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.730306Z",
     "start_time": "2018-08-12T19:37:22.981131Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nlp_pipe = supervised_nlp(LinearSVC(), nlp)\n",
    "nlp_pipe.fit(ng_train_data, ng_train_targets)\n",
    "nlp_pipe.score(ng_test_data, ng_test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised: Topic Modeling\n",
    "\n",
    "We don't want to make a prediction with this example, simply to find topics and have the ability to cast our data into the \"topic space\" from the \"word space.\" With this in mind, we'll add a transform feature and also the ability to print out the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.740426Z",
     "start_time": "2018-08-12T19:37:35.732734Z"
    }
   },
   "outputs": [],
   "source": [
    "class topic_modeling_nlp:\n",
    "    \n",
    "    def __init__(self, model, preprocessing_pipeline=None):\n",
    "        \"\"\"\n",
    "        A pipeline for doing supervised nlp. Expects a model and creates\n",
    "        a preprocessing pipeline if one isn't provided.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self._is_fit = False\n",
    "        if not preprocessing_pipeline:\n",
    "            self.preprocessor = nlp_preprocessor()\n",
    "        else:\n",
    "            self.preprocessor = preprocessing_pipeline\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Trains the vectorizer and model together using the \n",
    "        users input training data.\n",
    "        \"\"\"\n",
    "        self.preprocessor.fit(X)\n",
    "        train_data = self.preprocessor.transform(X)\n",
    "        self.model.fit(train_data)\n",
    "        self._is_fit = True\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Makes a prediction on the data provided by the users using the \n",
    "        preprocessing pipeline and provided model.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        test_data = self.preprocessor.transform(X)\n",
    "        preds = self.model.transform(test_data)\n",
    "        return preds\n",
    "    \n",
    "    def print_topics(self, num_words=10):\n",
    "        \"\"\"\n",
    "        A function to print out the top words for each topic\n",
    "        \"\"\"\n",
    "        feat_names = self.preprocessor.vectorizer.get_feature_names()\n",
    "        for topic_idx, topic in enumerate(self.model.components_):\n",
    "            message = \"Topic #%d: \" % topic_idx\n",
    "            message += \" \".join([feat_names[i]\n",
    "                                 for i in topic.argsort()[:-num_words - 1:-1]])\n",
    "            print(message)\n",
    "            \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+\".mdl\",'wb'))\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:35.760623Z",
     "start_time": "2018-08-12T19:37:35.743417Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "cleaning_pipe = nlp_preprocessor(vectorizer=cv)\n",
    "topic_chain = topic_modeling_nlp(TruncatedSVD(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.687204Z",
     "start_time": "2018-08-12T19:37:35.762876Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.734831Z",
     "start_time": "2018-08-12T19:37:36.689485Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_chain.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap out the model for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:36.741910Z",
     "start_time": "2018-08-12T19:37:36.737246Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "topic_chain = topic_modeling_nlp(LatentDirichletAllocation(n_components=15), preprocessing_pipeline=cleaning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T19:37:42.475762Z",
     "start_time": "2018-08-12T19:37:36.745876Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_chain.fit(ng_train_data)\n",
    "topic_chain.transform(ng_train_data).shape\n",
    "topic_chain.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca3",
   "language": "python",
   "name": "coca3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
