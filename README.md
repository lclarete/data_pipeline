# Next steps:

Currently working on the pre-processing-nlp

* Review the pre-processing-nlp directory
* Figure out how to use it in a simple way
* Write clear instructions of how to use it
* Create a file example of how to clean the text
* Apply it in the coursera NLP course

Then, go to modeling (classification):
* Coursera Course: apply Logistic Regression;
* Bayesian classification


# Data Pipeline

After feeling stuck for a whole day, I finally get this rep. 
I went over my short-whole coding experience to come up with the basic elements I need daily in order to perform my job.

I'm still insecure if it is the best structure to build, but I'm going to follow this one anyway. The reason for this feeling is the pressure of my first attempt to organize an entire data pipeline for my self. The goal is to use this format to gether codes related to each of its steps. 
I'll be working in a daily basis to improve it.

Levels:
- Get data: from APIs, scrapers, databases, files
- EDA: exploratory data analysis
- Preprocessing: cleaning and normalizing
- Modeling
- Plot

This mind map has helped me with the structural concept.
https://miro.com/app/board/o9J_koNsYBo=/

For now, focused on NLP.

# Linguistics and NLP
This is a repository to study NLP -- also my way to organize my coding learning.


Theory readings:
* On Chomsky and the Two Cultures of Statistical Learning
http://norvig.com/chomsky.html
* Michael Silverstein: helped define the field of sociolinguistics
> * Language in Culture: The Semiotics of Interaction (Masterclass)


Current resources:
* NLP coursera couse by FastAI: https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ungradedLab/TXtyC/natural-language-preprocessing
* FastAI github rep: https://github.com/fastai/course-nlp



Tools to get used to:
* http://www.nltk.org/
* https://developer.twitter.com/en/docs
* https://developer.twitter.com/en/developer-terms/policy
* https://brightplanet.com/2013/06/25/twitter-firehose-vs-twitter-api-whats-the-difference-and-why-should-you-care/





